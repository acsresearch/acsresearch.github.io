<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="/_next/static/css/dee7d9afc25d60d3.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/b85be9e705f3c91e.css" data-precedence="next"/><title>Alignment of Complex Systems Research Group – ACS Research Program</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/icon.png"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" noModule=""></script></head><body class="font-serif"><!--$--><!--/$--><div class="h-screen"><header class="Header_header__9U1DO Header_open-false__k_3uY"><div class="h-0 relative"><h1 class="undefined Header_abbr-other__oipRl"><a href="/">ACS</a></h1></div><span class="Header_burger__pz0eU">☰</span><nav class="Header_nav__gRhxM"><a href="/about">ABOUT</a><span class="Header_dotsep__mzEz2">•</span><a href="/research">RESEARCH</a><span class="Header_dotsep__mzEz2">•</span><a href="/team">TEAM</a><span class="Header_dotsep__mzEz2">•</span><a href="/contact">CONTACT</a></nav></header><div class="container prose Content_content__2uTaC"><h1 class="pt-8">ACS Research Program</h1><div><p>Our research seeks to build towards the following goals:</p>
<ul>
<li>A science of the space of intelligent systems</li>
<li>Naturalised theories of agency, including theories that account for the multi-scale nature of agentic behaviour</li>
<li>Nuanced accounts of how humans reason, act and value as it pertains to solving the AI alignment problem</li>
<li>An understanding of the underlying principles that govern collective behaviour between humans, between humans and AIs, and between AI systems</li>
</ul>
<h2>A science of the space of intelligent systems</h2>
<p>We seek to build systematic understanding of which properties of intelligent behaviour are universal, convergent or local across a wide range of systems, scales and substrates. This understanding forms the basis for asking the right sorts of questions about the risks, potentials and design imperatives of advanced AI systems.</p>
<p>To this end, we draw on a range of sophisticated thinking that has already been done, including in evolutionary biology, cognitive science, statistical physics, economics, ecology, cybernetics and information theory. By integrating and build on these traditions, we aim to better understand the trajectory space for advanced AI systems.</p>
<h2>Hierarchical Agency</h2>
<p>Over the past 100 or so years, a large amount of maths has been developed (most of it under the name of Game Theory) to help us describe the <em>relations between agents at the same level of analysis.</em> At the same time, many systems have several levels of analysis at which their behaviour could be sensibly described and explained. For instance, we can usefully model a company as an agent, or its employees; a social movement, or its followers; a nation-state, or its political class; etc.</p>
<p>At ACS, we aim to develop a conceptual framework, which can help us reason about the <em>relations between agents at different levels of analysis,</em> i.e. between superagents and their subagents. What we want is a formalism which is good for thinking about both upward and downward intentionality - something akin to ‘vertical game theory’ or a ‘theory of hierarchical agency’. We believe this understanding is critical to building and integrating AI systems into human socio-economic and political structures in a way that is safe and aligned.</p>
<h2>Realistic Accounts of how Humans Reason and Value</h2>
<p>‘Alignment’ or ‘safety’ are properties defined at the interface between a system and its environment, rather than properties intrinsic to a system in isolation. As such, it matters to understand the structural and functional properties of both the system that is to be aligned, as well as what we are aligning it to. However, we could pick out several plausible candidates as targets for what we want to align AI systems to–from individual humans to human groups. In addition, humans, importantly, are not accurately described by fixed or latent utility and belief functions, as the classical rational agent model suggests. Overall, current theorising on the appropriate targets of alignment and their structural properties is inadequate for understanding the subtleties that arise when tackling the problem of AI alignment.</p>
<p>ACS works on developing AI alignment proposals which are based on a realistic understanding of how humans reason and value in practice and which recognize the hierarchical relationships between these target systems is critical to solving the problem and not something which can be postponed or delegated to AI systems.</p>
<h2>Ecosystems of Intelligence</h2>
<p>Consider not just one powerful AGI, or several, but an entire ecosystem of different AI and human-AI systems and services varying in power, specialisation, and agency. From here, we can investigate questions about convergent or contingent properties of such ecosystems, what shapes their trajectory through time, as well as their strategic implications. For example, a period where human-AI teams are more capable than either humans or AI systems on their own might provide a critical window during which we can shape the nature of subsequent, more powerful systems and the institutions and protocols which shape their interactions. ACS’s work explores problems of AI risk and safety from the ecosystems perspective.</p></div></div></div><script src="/_next/static/chunks/webpack-3bcda1720f2c2906.js" async=""></script><script src="/_next/static/chunks/ba97af87-437b650d2319c70e.js" async=""></script><script src="/_next/static/chunks/887-bc5a4c1ce8ac1c4f.js" async=""></script><script src="/_next/static/chunks/main-app-c88853ead6726c9d.js" async=""></script></body></html><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/dee7d9afc25d60d3.css\",{\"as\":\"style\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/b85be9e705f3c91e.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"4:I{\"id\":\"2163\",\"chunks\":[\"272:static/chunks/webpack-3bcda1720f2c2906.js\",\"549:static/chunks/ba97af87-437b650d2319c70e.js\",\"887:static/chunks/887-bc5a4c1ce8ac1c4f.js\"],\"name\":\"\",\"async\":false}\n6:I{\"id\":\"5250\",\"chunks\":[\"272:static/chunks/webpack-3bcda1720f2c2906.js\",\"549:static/chunks/ba97af87-437b650d2319c70e.js\",\"887:static/chunks/887-bc5a4c1ce8ac1c4f.js\"],\"name\":\"\",\"async\":false}\n7:\"$Sreact.strict_mode\"\n8:\"$Sreact.suspense\"\n9:I{\"id\":\"6791\",\"chunks\":[\"185:static/chunks/app/layout-dc2c27e2c215d40a.js\"],\"na"])</script><script>self.__next_f.push([1,"me\":\"\",\"async\":false}\na:I{\"id\":\"1647\",\"chunks\":[\"272:static/chunks/webpack-3bcda1720f2c2906.js\",\"549:static/chunks/ba97af87-437b650d2319c70e.js\",\"887:static/chunks/887-bc5a4c1ce8ac1c4f.js\"],\"name\":\"\",\"async\":false}\nb:I{\"id\":\"1664\",\"chunks\":[\"272:static/chunks/webpack-3bcda1720f2c2906.js\",\"549:static/chunks/ba97af87-437b650d2319c70e.js\",\"887:static/chunks/887-bc5a4c1ce8ac1c4f.js\"],\"name\":\"\",\"async\":false}\nc:I{\"id\":\"6615\",\"chunks\":[\"849:static/chunks/849-3b3ce6b1884af69b.js\",\"991:static/chunks/app/posts/page-"])</script><script>self.__next_f.push([1,"1caf10189b9535a1.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/dee7d9afc25d60d3.css\",\"precedence\":\"next\"}]],[\"$\",\"$L4\",null,{\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/research\",\"initialTree\":[\"\",{\"children\":[[\"path\",\"research\",\"c\"],{\"children\":[\"__PAGE__?{\\\"path\\\":[\\\"research\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[\"$L5\",null],\"globalErrorComponent\":\"$6\",\"notFound\":[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"$7\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"font-serif\",\"children\":[[\"$\",\"$8\",null,{\"children\":[\"$\",\"$L9\",null,{}]}],[\"$undefined\",[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]]]}]}]}],\"asNotFound\":false,\"children\":[[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"$7\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"font-serif\",\"children\":[[\"$\",\"$8\",null,{\"children\":[\"$\",\"$L9\",null,{}]}],[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"asNotFound\":false,\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",[\"path\",\"research\",\"c\"],\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"asNotFound\":false,\"childProp\":{\"current\":[[\"$\",\"div\",null,{\"className\":\"h-screen\",\"children\":[[\"$\",\"$Lc\",null,{\"root\":false}],[\"$\",\"div\",null,{\"className\":\"container prose Content_content__2uTaC\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"pt-8\",\"children\":\"ACS Research Program\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cp\u003eOur research seeks to build towards the following goals:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eA science of the space of intelligent systems\u003c/li\u003e\\n\u003cli\u003eNaturalised theories of agency, including theories that account for the multi-scale nature of agentic behaviour\u003c/li\u003e\\n\u003cli\u003eNuanced accounts of how humans reason, act and value as it pertains to solving the AI alignment problem\u003c/li\u003e\\n\u003cli\u003eAn understanding of the underlying principles that govern collective behaviour between humans, between humans and AIs, and between AI systems\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eA science of the space of intelligent systems\u003c/h2\u003e\\n\u003cp\u003eWe seek to build systematic understanding of which properties of intelligent behaviour are universal, convergent or local across a wide range of systems, scales and substrates. This understanding forms the basis for asking the right sorts of questions about the risks, potentials and design imperatives of advanced AI systems.\u003c/p\u003e\\n\u003cp\u003eTo this end, we draw on a range of sophisticated thinking that has already been done, including in evolutionary biology, cognitive science, statistical physics, economics, ecology, cybernetics and information theory. By integrating and build on these traditions, we aim to better understand the trajectory space for advanced AI systems.\u003c/p\u003e\\n\u003ch2\u003eHierarchical Agency\u003c/h2\u003e\\n\u003cp\u003eOver the past 100 or so years, a large amount of maths has been developed (most of it under the name of Game Theory) to help us describe the \u003cem\u003erelations between agents at the same level of analysis.\u003c/em\u003e At the same time, many systems have several levels of analysis at which their behaviour could be sensibly described and explained. For instance, we can usefully model a company as an agent, or its employees; a social movement, or its followers; a nation-state, or its political class; etc.\u003c/p\u003e\\n\u003cp\u003eAt ACS, we aim to develop a conceptual framework, which can help us reason about the \u003cem\u003erelations between agents at different levels of analysis,\u003c/em\u003e i.e. between superagents and their subagents. What we want is a formalism which is good for thinking about both upward and downward intentionality - something akin to ‘vertical game theory’ or a ‘theory of hierarchical agency’. We believe this understanding is critical to building and integrating AI systems into human socio-economic and political structures in a way that is safe and aligned.\u003c/p\u003e\\n\u003ch2\u003eRealistic Accounts of how Humans Reason and Value\u003c/h2\u003e\\n\u003cp\u003e‘Alignment’ or ‘safety’ are properties defined at the interface between a system and its environment, rather than properties intrinsic to a system in isolation. As such, it matters to understand the structural and functional properties of both the system that is to be aligned, as well as what we are aligning it to. However, we could pick out several plausible candidates as targets for what we want to align AI systems to–from individual humans to human groups. In addition, humans, importantly, are not accurately described by fixed or latent utility and belief functions, as the classical rational agent model suggests. Overall, current theorising on the appropriate targets of alignment and their structural properties is inadequate for understanding the subtleties that arise when tackling the problem of AI alignment.\u003c/p\u003e\\n\u003cp\u003eACS works on developing AI alignment proposals which are based on a realistic understanding of how humans reason and value in practice and which recognize the hierarchical relationships between these target systems is critical to solving the problem and not something which can be postponed or delegated to AI systems.\u003c/p\u003e\\n\u003ch2\u003eEcosystems of Intelligence\u003c/h2\u003e\\n\u003cp\u003eConsider not just one powerful AGI, or several, but an entire ecosystem of different AI and human-AI systems and services varying in power, specialisation, and agency. From here, we can investigate questions about convergent or contingent properties of such ecosystems, what shapes their trajectory through time, as well as their strategic implications. For example, a period where human-AI teams are more capable than either humans or AI systems on their own might provide a critical window during which we can shape the nature of subsequent, more powerful systems and the institutions and protocols which shape their interactions. ACS’s work explores problems of AI risk and safety from the ecosystems perspective.\u003c/p\u003e\"}}]]}]]}],null],\"segment\":\"__PAGE__?{\\\"path\\\":[\\\"research\\\"]}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/b85be9e705f3c91e.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"path\",\"research\",\"c\"]},\"styles\":[]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"5:[[[\"$\",\"meta\",null,{\"charSet\":\"utf-8\"}],[\"$\",\"title\",null,{\"children\":\"Alignment of Complex Systems Research Group – ACS Research Program\"}],null,null,null,null,null,null,null,null,null,[\"$\",\"meta\",null,{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],null,null,null,null,null,null,null,null,null,null,[]],[null,null,null,null],null,null,[null,null,null,null,null],null,null,null,null,[null,[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/icon.png\"}]],[],null]]\n"])</script>