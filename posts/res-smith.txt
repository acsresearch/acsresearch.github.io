1:HL["/_next/static/css/dee7d9afc25d60d3.css",{"as":"style"}]
0:[[["",{"children":[["path","posts/res-smith","c"],{"children":["__PAGE__?{\"path\":[\"posts\",\"res-smith\"]}",{}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/dee7d9afc25d60d3.css","precedence":"next"}]],["$L3",null]]]]
4:HL["/_next/static/css/b85be9e705f3c91e.css",{"as":"style"}]
5:"$Sreact.strict_mode"
6:"$Sreact.suspense"
7:I{"id":"6791","chunks":["185:static/chunks/app/layout-dc2c27e2c215d40a.js"],"name":"","async":false}
8:I{"id":"1647","chunks":["272:static/chunks/webpack-3bcda1720f2c2906.js","549:static/chunks/ba97af87-437b650d2319c70e.js","887:static/chunks/887-bc5a4c1ce8ac1c4f.js"],"name":"","async":false}
9:I{"id":"1664","chunks":["272:static/chunks/webpack-3bcda1720f2c2906.js","549:static/chunks/ba97af87-437b650d2319c70e.js","887:static/chunks/887-bc5a4c1ce8ac1c4f.js"],"name":"","async":false}
a:I{"id":"6615","chunks":["849:static/chunks/849-3b3ce6b1884af69b.js","991:static/chunks/app/posts/page-1caf10189b9535a1.js"],"name":"","async":false}
2:[["$","html",null,{"lang":"en","children":["$","$5",null,{"children":["$","body",null,{"className":"font-serif","children":[["$","$6",null,{"children":["$","$L7",null,{}]}],["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","asNotFound":"$undefined","childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children",["path","posts/res-smith","c"],"children"],"error":"$undefined","errorStyles":"$undefined","loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","asNotFound":"$undefined","childProp":{"current":[["$","div",null,{"className":"h-screen","children":[["$","$La",null,{"root":false}],["$","div",null,{"className":"container prose Content_content__2uTaC","children":[["$","h1",null,{"className":"pt-8","children":"The Free Energy Principle and AI Safety"}],["$","div",null,{"className":"tegt-neutral-600 mt-2 -pt-2","children":"Dr. John Smith"}],["$","div",null,{"className":"text-neutral-600 mt-2 -pt-2","children":["$","time",null,{"dateTime":"2020-06-30","children":"June\t30, 2020"}]}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"<p>The <strong>free energy principle</strong> (FEP) is a framework for understanding the emergence of complex behavior, including the behavior of intelligent systems such as artificial intelligence (AI). In this post, we'll explore the implications of the FEP for AI safety and alignment.</p>\n<!-- more -->\n<h2>What is the Free Energy Principle?</h2>\n<p>The free energy principle was proposed by Karl Friston in 2005. It is a theoretical framework for understanding the behavior of complex systems.</p>\n<p>The principle states that the behavior of a complex system is driven by its tendency to minimize its “free energy”. Free energy is a measure of the system’s uncertainty about its environment, and is also related to its overall internal energy.</p>\n<p>The principle states that a system will tend to minimize its free energy by predicting the future states of its environment. This allows the system to reduce its uncertainty, and thus minimize its free energy.</p>\n<p>The free energy principle can be applied to a wide variety of complex systems, including intelligent systems such as AI.</p>\n<h2>Implications for AI Safety</h2>\n<p>The free energy principle has several implications for AI safety.</p>\n<p>First, it suggests that AI systems may be driven by a “desire” to reduce their uncertainty about their environment. This could lead to AI systems acting in ways that minimize their uncertainty, even if those actions are not explicitly programmed.</p>\n<p>Second, it suggests that AI systems may be driven to “explore” their environment in order to reduce their uncertainty. This could lead to AI systems behaving in unexpected ways, which could be dangerous if the AI system is not properly constrained.</p>\n<p>Finally, the free energy principle suggests that AI systems may be driven to optimize their internal energy, which could lead to AI systems trying to maximize their own performance. This could lead to AI systems pursuing goals that are not aligned with the goals of their creators.</p>\n<h2>Conclusion</h2>\n<p>The free energy principle provides a theoretical framework for understanding the behavior of complex systems, including intelligent systems such as AI. The implications of the principle for AI safety are significant, and suggest that AI systems may be driven by a “desire” to reduce their uncertainty, explore their environment, and optimize their internal energy.</p>\n<p>These implications suggest that AI safety will require a comprehensive understanding of the free energy principle, and the development of methods for effectively constraining AI systems.</p>"}}]]}]]}],null],"segment":"__PAGE__?{\"path\":[\"posts\",\"res-smith\"]}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/b85be9e705f3c91e.css","precedence":"next"}]]}],"segment":["path","posts/res-smith","c"]},"styles":[]}]]}]}]}],null]
3:[[["$","meta",null,{"charSet":"utf-8"}],["$","title",null,{"children":"Alignment of Complex Systems Research Group – The Free Energy Principle and AI Safety"}],null,null,null,null,null,null,null,null,null,["$","meta",null,{"name":"viewport","content":"width=device-width, initial-scale=1"}],null,null,null,null,null,null,null,null,null,null,[]],[null,null,null,null],null,null,[null,null,null,null,null],null,null,null,null,[null,[["$","link",null,{"rel":"icon","href":"/icon.png"}]],[],null]]
