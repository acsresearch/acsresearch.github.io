2:I[2058,["807","static/chunks/807-2fc36657cdfdd175.js","101","static/chunks/101-6320b452af5c9a08.js","931","static/chunks/app/page-2f08b52657019d14.js"],"default"]
4:I[9986,[],""]
6:I[3582,[],""]
7:"$Sreact.strict_mode"
8:"$Sreact.suspense"
9:I[1646,["185","static/chunks/app/layout-5274a54cbda7604f.js"],"default"]
3:T4393,<img src="interlab-intro/logo3-cut.webp" alt="InterLab Logo" width="350" style="margin: 30pt auto" />
<p><em>This post introduces InterLab, a toolkit for experiments with multi-agent interaction. We plan to release more posts on the overall project, technical details, design considerations, and concrete research projects and ideas over the next few weeks to months.</em></p>
<p><em>This post focuses on the motivation behind the project and touches upon more high-level considerations; if you want to jump to the implementation itself or want to start experimenting, you can jump directly to the <a href="#getting-started">Getting started section</a>, to the <a href="https://github.com/acsresearch/interlab">InterLab GitHub repo</a> with project overview and further links, or explore the <a href="https://colab.research.google.com/github/acsresearch/interlab/blob/main/notebooks/car_negotiation_colab.ipynb">example Google Colab notebook</a>.</em></p>
<h2>Motivation</h2>
<p>The research agenda of ACS is primarily focused on understanding complex interactions of humans and AI agents, both on individual level and between systems or institutions, and both theoretically and empirically.</p>
<p>Future going well in our view depends not just on the narrow ability to point one AI system to some well specified goal, but broadly on a complex system composed of both AIs and humans to develop in a way which is conducive to human flourishing. This points to a somewhat different set of questions than traditional AI safety, including problems such as "how to deal with the misalignment between individual humans, or between humans and institutions?", "how to avoid AIs amplifying conflict?", "how will institutions running on AI cognition (rather than human cognition) work?", "how do aggregate multi-agent entities evolve?", or "what happens if you replace part of human-human interactions in the society with AI-AI interactions?". </p>
<p>While many of these questions likely require a better theoretical and conceptual understanding, it is also possible to study them empirically, using LLMs and LLM-based agents, which can also inform our models and intuitions.</p>
<ul>
<li>
<p>We may build more comprehensive language model evaluations for near-term alignment, in particular in the direction of multi-agent evaluations—this is indeed one of the goals of InterLab.</p>
</li>
<li>
<p>We may learn about strategies for resolving conflicts and disagreements, and robust cooperation, as well as models of manipulation and coercion, in particular under information and power imbalances.</p>
</li>
<li>
<p>We may create new technologies for human coordination, cooperation and empowerment, such as negotiation aids or aids for solving internal conflicts in individual humans.</p>
</li>
<li>
<p>Multi-agent systems of humans and AIs come with a specific and understudied set of risks (longer report forthcoming).</p>
</li>
<li>
<p>Better empirical understanding of systems of interacting LLMs can help us better understand <a href="https://www.lesswrong.com/posts/b9sGz74ayftqPBDYv/the-space-of-systems-and-the-space-of-maps">the space of intelligent systems</a> occupied by collective intelligences and superagents.</p>
</li>
<li>
<p>There is some risk of over-updating our models and intuitions based on the current AI systems that needs to be taken into account, but alignment theory developed more in touch with experiments seems like a useful direction.</p>
</li>
</ul>
<p>Another intuition behind this work is the insight that sometimes it is easier to understand or predict the behavior of a system of agents as a whole and based on simplified models, rather than to e.g. model the individuals accurately and then model the system primarily as a collection of individuals. For example, modeling the flow of passengers in a metropolitan transit system is notably easier than understanding individual humans and their reasons why they move in some particular ways. (In fact, some systems in human civilization are specifically designed to avoid the outcome being too influenced by properties of individuals, e.g. markets and the justice system.)</p>
<p>Empirical language model research and experimentation are taking off quickly both within industry and mainstream ML and other fields (social sciences, fairness) and it is hardly a neglected area as a whole. However, we believe that AI alignment in fact has different priorities and hypotheses of interest than mainstream research, and while it is not trivial to distinguish those questions, there is important differential work to be done here.</p>
<h2>Agent Interaction Lab</h2>
<p>As a part of our empirical research, we have been developing an agent interaction lab  (InterLab), a Python software framework to make this research easier and more efficient for us and others from the alignment community working on problems in this area. Over the last 6 months, we have been developing the framework based on the experience we and other users had with it, and we now want to share it with the research community, both as a helpful tool, a modular common base for collaboration, and not least to get more information on design and desiderata.</p>
<h3>Design and desiderata</h3>
<p>What do we want from a framework to study and explore agent interaction?</p>
<ul>
<li>
<p>In its core, a modular system of actors, environments, memory systems, language models, games and scenarios, with a simple and convenient API, composability into scaffolds and hierarchies, with pre-built basic components and easy extensibility. InterLab is designed with (PO)MDPs and game theory as the underlying model and can also work with non-textual data as actions, observations etc., as well as querying agents and LLMs for typed structured data.</p>
</li>
<li>
<p>On the usability side, we want general and convenient ways to monitor, log, and debug agent interactions, and even play some of the scenarios. For that we provide structured experiment logging with interactive inspection and exploration tools. Since LLM experiments often generate an extensive amount of text and data—especially when scaffolded—figuring out what went wrong on the language level can be a substantial task. Our logs are hierarchical, allowing both high-level overview and getting into details of each LLM API call made.</p>
</li>
<li>
<p>We also want both of the above to be easy to set up and use, whether in a Jupyter notebook, in Google Colab, or running batches of experiments with e.g. Hydra. InterLab also provides an interactive interface to play as one or more of the actors—for debugging, interactive experiments, or just for fun. The log system and interactive interface also support logging pictures, plots, tables, formatted rich text etc.</p>
</li>
</ul>
<h3>InterLab today</h3>
<p>InterLab currently covers all of the above, as well as some additional LLM-specific and interactive functionality:</p>
<ul>
<li>
<p>General LLM agent implementations: general one-shot LLM agent and chain-of-thought agents.</p>
</li>
<li>
<p>Memory systems for agents, including an associative memory based on embeddings, and a summarizing memory (summarizing older and less terse memories to stay in the context token limit).</p>
</li>
<li>
<p>Robust querying for typed data (e.g. python data-classes) via JSON, schemas, and examples.</p>
</li>
<li>
<p>Web-based interactive player interface with auto-generated interface given the input schema for easier testing—no need for custom web interface or custom parsers for early experiments.</p>
</li>
<li>
<p>The state of the environment can be copied and stored, for checkpointing as well as to experiment with counterfactuals, local response optimization, and other algorithms.</p>
</li>
<li>
<p>Interoperability with LangChain and all the models it wraps, including caching (though we do not use it for much else than that).</p>
</li>
</ul>
<p>While most of the items above are relatively easy to implement, and various parts have been implemented in other places, we hope that a large portion of the value of InterLab is it provides a sensible structure for experiments from start to finish along with best practices and a shared interface for research collaborations, rather than just a collection of functionality. Compared to most frameworks for LLM scaffolding, InterLab has a stronger emphasis on usability for research and analysis rather than on production use and capabilities.</p>
<h3>Roadmap and future plans</h3>
<p>Beyond the above, there are few more things we want from the framework and that we keep in mind while designing it. Some of those are work in progress, others can be found among the experimental notebooks, others only in our private experiments.</p>
<ul>
<li>
<p>Branching out experiments to explore counterfactual developments. The base functionality is available, but there is more possible tooling to be built and use-cases to be explored.</p>
</li>
<li>
<p>Affordance specification—including both environment actions and available tools/plugins (e.g. specific memory recall, code evaluation, or external tools) with minimal and clean design. We have a preliminary design that did not make it to the 0.4 release. This would also cover at least some cases of action-space specification in game-theoretic settings.</p>
</li>
<li>
<p>Meta-scenarios—scenarios with agents observing and interacting with another scenario, e.g. detecting ongoing manipulation in an observed agent interaction or other emergent dynamics or risks. Can be extended to richer interactions, e.g. meta-actors inspecting the mesa-actors' memories at different time-points, or interacting with their snapshot at various times. This depends on in-progress features (de/serialization, affordances), then needs scenario and idea development, and then implementation of the relevant meta-level affordances.</p>
</li>
<li>
<p>Game-theoretic scenarios and algorithms. While these so far do not seem central to LLM agent interactions (as simple games like PD often do not capture the complex nature of language interactions, and LLMs do not seem to be good at resolving game-theoretic situations strategically), there are both theoretical and empirical directions to explore.</p>
</li>
<li>
<p>Advanced algorithms using counterfactual exploration would include e.g. an actor using MCTS-based action search for an ideal response. This ties well into research related to manipulation and information asymmetry.</p>
</li>
<li>
<p>More tooling to use scenarios as evaluation benchmarks (evals) and red-teaming, e.g. interactive evaluation harnesses. This seems like a potentially high-value downstream effect of the framework.</p>
</li>
<li>
<p>Tooling for discord and other chat and interface integrations. While not a priority for academic research, it would be useful for building and exploring positive-value tools to improve cooperation or information processing, as well as just for internal experimentation and intuition building in an organization.</p>
</li>
<li>
<p>Implementing nested environments, including hierarchically nested rooms or locations, globally coordinated simulation time, and dynamic agent interactions. With the addition of action affordances, this should be buildable from existing blocks.</p>
</li>
<li>
<p>More ambitious goals include e.g. incorporating activation vectors and influence vectors. Requires careful experiment design and needs a lot of technical work with the models.</p>
</li>
</ul>
<h2>Concrete LLM interaction experiments</h2>
<p>Apart from these general and technical directions, we also have ideas and proposals for empirical research and exploration into agent interactions, game theory, and cooperation, that would be well suited for the toolkit we are developing—some of them also as potential evals. We intend to pursue some of these ourselves, but we hope others will join and make progress faster. The list of potentially exciting avenues is definitely beyond our current capacity, and we are very open to collaboration.</p>
<p>We will list more  of these ideas in a future post, but here are a few examples of actionable experiments:</p>
<ul>
<li>
<p>AIs negotiating on behalf of humans: Alice and Bob delegate a bargaining situation to their AI assistants, who then negotiate. Empirically, how does this work? In open-ended environments, can AIs find Pareto-improvements over humans negotiating in a limited time? Do alignment methods like RLHF make AIs better or worse at negotiation? Do "smarter" AIs perform better? How much does prompting to be more aggressive or strategic actually help the outcome? Can we characterize and recognize negotiations which are "fair", and not eg. manipulative?</p>
</li>
<li>
<p>Runtime deliberation over problematic user requests: Currently, efforts to align LLMs usually try to "bake in" morality and policies into model weights. This is a bit like trying to teach a human to have infallible moral intuition, giving instantly correct answers to every edgy or complicated problem. Attempts to this usually end up with models being too cautious, refusing reasonable requests. What if, instead, in case of tricky or borderline requests we make AI agents deliberate or debate in runtime what a good answer is and if it should be provided to the user? (Perhaps similarly to humans, who when faced with a tricky moral or otherwise complex situation engage in explicit System 2 deliberation, enacting internal arguments and simulations.)</p>
</li>
<li>
<p>LLMs as automated argument-mappers: Make a group of LLM agents take some positions in a disagreement and have them explore the argument space for possible cruxes, points needing clarification, weak and strong pieces of evidence, etc. Can the result help humans understand a disagreement better? Or make meetings more productive and steer debates toward cruxes?</p>
</li>
</ul>
<p><a name="getting-started"></a></p>
<h2>The implementation and how to get started</h2>
<p><strong>InterLab</strong> is a Python package licenced under MIT and developed primarily at <a href="https://github.com/acsresearch/interlab">GitHub</a>. We recently published version 0.4.x with a redesign and simplification of the core API based on our experience and plans, and while it may still change, we now intend to focus more on stability and compatibility of the framework, updating functionality in a more modular fashion.</p>
<p>The <a href="https://acsresearch.org/interlab/stable/">documentation and user guide</a> gives an overview of the main components and their usage, though it is currently still missing some of the latest development. For technical details, we recommend <a href="http://127.0.0.1:8000/interlab/api/">API documentation</a> or checking out the core package sources, as well as examples.</p>
<p>The best way to see how you can use InterLab in your experiments is to look at our <a href="https://github.com/acsresearch/interlab/blob/main/notebooks/">example notebooks</a>. We recommend starting with the <a href="https://github.com/acsresearch/interlab/blob/main/notebooks/car_negotiation.ipynb">Car sale negotiation notebook</a>, which you can also <a href="https://colab.research.google.com/github/acsresearch/interlab/blob/main/notebooks/car_negotiation_colab.ipynb">open in Google Colab</a> (a free Colab account is sufficient) and experiment right away. You will still need to provide your own API keys for OpenAI or some other LLM provider to run the LLM experiments.</p>
<p>The browser lets you explore the experiment run on a high level as well as dig into details down to the actual LLM calls—which is especially handy when using multiple calls for internal actor deliberation, compound (hierarchical) actors, or other scaffolding.
Here is a screenshot of the trace browser showing some high-level details of a simple scenario:</p>
<img src="interlab-intro/context-browser-1.png" style="margin: 40pt auto; max-width: 90%;"/>
<h3>How to get involved</h3>
<p>We are interested in collaboration on various agent interaction experiments and research, as well as in sharing models and ideas. We may also be able to mentor some projects, though we are constrained on capacity and these projects may be best pursued within one of the alignment research fellowships (e.g. MATS, PIBBSS).</p>
<p>We want to develop the InterLab framework to be maximally useful to the alignment research community, and we would be interested in your feedback, suggestions for improvement, potential and actual use-cases, as well as bug-reports and issues. If you use InterLab in your research or just create a small experiment, let us know.</p>
<p>We also welcome code contributions—whether bug-fixes or improvements to tooling and integrations, or contributing scenarios, actors, and examples in the interlab_zoo package. The development happens at InterLab GitHub, and the preferred way to contribute is opening a PR, starting discussion in an issue, or in some cases just reaching out to us directly.</p>
<hr>
<p><em>We want to thank Nora Amman and Clem von Stengel for feedback on the writeup and the project overall, and Mihaly Barasz, Jason Hoelscher-Obermaier, Connor Pancoast, Anna Gajdová, and Jiří Nádvorník for feedback and assistance designing and developing the project.</em></p>5:["path","posts/interlab-intro","c"]
0:["o0Ke6ByuUwFcme3qD0eZc",[[["",{"children":[["path","posts/interlab-intro","c"],{"children":["__PAGE__?{\"path\":[\"posts\",\"interlab-intro\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["path","posts/interlab-intro","c"],{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"h-screen","children":[["$","$L2",null,{"root":false}],["$","div",null,{"className":"container prose Content_content__bGfAW","children":[["$","h1",null,{"className":"pt-8","children":"InterLab – a toolkit for experiments with multi-agent interactions"}],["$","div",null,{"className":"tegt-neutral-600 mt-2 -pt-2","children":"Tomáš Gavenčiak, Ada Böhm, Jan Kulveit"}],["$","div",null,{"className":"text-neutral-600 mt-2 -pt-2","children":["$","time",null,{"dateTime":"2024-01-21T00:00:00.000Z","children":"January\t21, 2024"}]}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$3"}}]]}],["$","footer",null,{"className":"footer-container","children":[["$","div",null,{"className":"footer-divider"}],["$","div",null,{"className":"footer-column-logo","children":["$","img",null,{"className":"footer-logo","src":"/cuni.png","alt":"CUNI Logo"}]}],["$","div",null,{"className":"footer-columns","children":[["$","div",null,{"className":"footer-subcolumn","children":["ACS research group is part of"," ",["$","a",null,{"href":"http://www.cts.cuni.cz/en","children":"Center for Theoretical Study"}]," at"," ",["$","a",null,{"href":"https://cuni.cz/en","children":"Charles University in Prague."}]]}],["$","div",null,{"className":"footer-subcolumn","children":[["$","div",null,{"className":"footer-contact-line","children":[["$","span",null,{"style":{"float":"left","padding":"0 0.5em 1.3em 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4L236.8 313.6c11.4 8.5 27 8.5 38.4 0L492.8 150.4c12.1-9.1 19.2-23.3 19.2-38.4c0-26.5-21.5-48-48-48H48zM0 176V384c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V176L294.4 339.2c-22.8 17.1-54 17.1-76.8 0L0 176z"}]}]}],"Husova 4",["$","br",null,{}],"110 00 Prague, CZ"]}],["$","div",null,{"className":"footer-contact-line","children":["$","a",null,{"href":"mailto:contact@acsresearch.org","children":[["$","span",null,{"style":{"padding":"0 0.5em 0 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M256 64C150 64 64 150 64 256s86 192 192 192c17.7 0 32 14.3 32 32s-14.3 32-32 32C114.6 512 0 397.4 0 256S114.6 0 256 0S512 114.6 512 256v32c0 53-43 96-96 96c-29.3 0-55.6-13.2-73.2-33.9C320 371.1 289.5 384 256 384c-70.7 0-128-57.3-128-128s57.3-128 128-128c27.9 0 53.7 8.9 74.7 24.1c5.7-5 13.1-8.1 21.3-8.1c17.7 0 32 14.3 32 32v80 32c0 17.7 14.3 32 32 32s32-14.3 32-32V256c0-106-86-192-192-192zm64 192a64 64 0 1 0 -128 0 64 64 0 1 0 128 0z"}]}]}],"contact@acsresearch.org"]}]}],["$","div",null,{"className":"footer-contact-line","children":["$","a",null,{"href":"https://twitter.com/acsresearchorg","children":[["$","span",null,{"style":{"padding":"0 0.5em 0 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"}]}]}],"@acsresearchorg"]}]}]]}]]}]]}]]}]],null],null]},["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/c70e259a19bc14f0.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},[["$","html",null,{"lang":"en","children":["$","$7",null,{"children":["$","body",null,{"className":"font-serif","children":[["$","$8",null,{"children":["$","$L9",null,{}]}],["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a6a7d77a57a1ff75.css","precedence":"next","crossOrigin":"$undefined"}]],"$La"]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Alignment of Complex Systems Research Group – InterLab – a toolkit for experiments with multi-agent interactions"}],["$","link","3",{"rel":"icon","href":"/icon.png"}]]
1:null
