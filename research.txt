1:HL["/_next/static/css/a3902502a8cdc05e.css",{"as":"style"}]
0:[[["",{"children":[["path","research","c"],{"children":["__PAGE__?{\"path\":[\"research\"]}",{}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a3902502a8cdc05e.css","precedence":"next"}]],["$L3",null]]]]
4:HL["/_next/static/css/b85be9e705f3c91e.css",{"as":"style"}]
5:"$Sreact.strict_mode"
6:I{"id":"1647","chunks":["272:static/chunks/webpack-e796d753467255ac.js","549:static/chunks/ba97af87-437b650d2319c70e.js","887:static/chunks/887-bc5a4c1ce8ac1c4f.js"],"name":"","async":false}
7:I{"id":"1664","chunks":["272:static/chunks/webpack-e796d753467255ac.js","549:static/chunks/ba97af87-437b650d2319c70e.js","887:static/chunks/887-bc5a4c1ce8ac1c4f.js"],"name":"","async":false}
8:I{"id":"7369","chunks":["849:static/chunks/849-3b3ce6b1884af69b.js","991:static/chunks/app/posts/page-e822cdbb07121eff.js"],"name":"","async":false}
2:[["$","html",null,{"lang":"en","children":["$","$5",null,{"children":["$","body",null,{"className":"font-serif","children":["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"template":["$","$L7",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","asNotFound":"$undefined","childProp":{"current":["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children",["path","research","c"],"children"],"error":"$undefined","errorStyles":"$undefined","loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"template":["$","$L7",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","asNotFound":"$undefined","childProp":{"current":[["$","div",null,{"className":"h-screen","children":[["$","$L8",null,{"root":false}],["$","div",null,{"className":"container prose Content_content__2uTaC","children":[["$","h1",null,{"children":"ACS Research Program"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"<p>Our research seeks to build towards the following goals:</p>\n<ul>\n<li>A science of the space of intelligent systems</li>\n<li>Naturalised theories of agency, including theories that account for the multi-scale nature of agentic behaviour</li>\n<li>Nuanced accounts of how humans reason, act and value as it pertains to solving the AI alignment problem</li>\n<li>An understanding of the underlying principles that govern collective behaviour between humans, between humans and AIs, and between AI systems</li>\n</ul>\n<h2>A science of the space of intelligent systems</h2>\n<p>We seek to build systematic understanding of which properties of intelligent behaviour are universal, convergent or local across a wide range of systems, scales and substrates. This understanding forms the basis for asking the right sorts of questions about the risks, potentials and design imperatives of advanced AI systems.</p>\n<p>To this end, we draw on a range of sophisticated thinking that has already been done, including in evolutionary biology, cognitive science, statistical physics, economics, ecology, cybernetics and information theory. By integrating and build on these traditions, we aim to better understand the trajectory space for advanced AI systems.</p>\n<h2>Hierarchical Agency</h2>\n<p>Over the past 100 or so years, a large amount of maths has been developed (most of it under the name of Game Theory) to help us describe the <em>relations between agents at the same level of analysis.</em> At the same time, many systems have several levels of analysis at which their behaviour could be sensibly described and explained. For instance, we can usefully model a company as an agent, or its employees; a social movement, or its followers; a nation-state, or its political class; etc.</p>\n<p>At ACS, we aim to develop a conceptual framework, which can help us reason about the <em>relations between agents at different levels of analysis,</em> i.e. between superagents and their subagents. What we want is a formalism which is good for thinking about both upward and downward intentionality - something akin to ‘vertical game theory’ or a ‘theory of hierarchical agency’. We believe this understanding is critical to building and integrating AI systems into human socio-economic and political structures in a way that is safe and aligned.</p>\n<h2>Realistic Accounts of how Humans Reason and Value</h2>\n<p>‘Alignment’ or ‘safety’ are properties defined at the interface between a system and its environment, rather than properties intrinsic to a system in isolation. As such, it matters to understand the structural and functional properties of both the system that is to be aligned, as well as what we are aligning it to. However, we could pick out several plausible candidates as targets for what we want to align AI systems to–from individual humans to human groups. In addition, humans, importantly, are not accurately described by fixed or latent utility and belief functions, as the classical rational agent model suggests. Overall, current theorising on the appropriate targets of alignment and their structural properties is inadequate for understanding the subtleties that arise when tackling the problem of AI alignment.</p>\n<p>ACS works on developing AI alignment proposals which are based on a realistic understanding of how humans reason and value in practice and which recognize the hierarchical relationships between these target systems is critical to solving the problem and not something which can be postponed or delegated to AI systems.</p>\n<h2>Ecosystems of Intelligence</h2>\n<p>Consider not just one powerful AGI, or several, but an entire ecosystem of different AI and human-AI systems and services varying in power, specialisation, and agency. From here, we can investigate questions about convergent or contingent properties of such ecosystems, what shapes their trajectory through time, as well as their strategic implications. For example, a period where human-AI teams are more capable than either humans or AI systems on their own might provide a critical window during which we can shape the nature of subsequent, more powerful systems and the institutions and protocols which shape their interactions. ACS’s work explores problems of AI risk and safety from the ecosystems perspective.</p>"}}]]}]]}],null],"segment":"__PAGE__?{\"path\":[\"research\"]}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/b85be9e705f3c91e.css","precedence":"next"}]]}],"segment":["path","research","c"]},"styles":[]}]}]}]}],null]
3:[[["$","meta",null,{"charSet":"utf-8"}],["$","title",null,{"children":"Alignment of Complex Systems Research Group – ACS Research Program"}],null,null,null,null,null,null,null,null,null,["$","meta",null,{"name":"viewport","content":"width=device-width, initial-scale=1"}],null,null,null,null,null,null,null,null,null,null,[]],[null,null,null,null],null,null,[null,null,null,null,null],null,null,null,null,null]
