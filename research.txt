2:I[2058,["807","static/chunks/807-2fc36657cdfdd175.js","884","static/chunks/app/%5B...path%5D/page-550865eddfc3af74.js"],"default"]
4:I[9986,[],""]
6:I[3582,[],""]
7:"$Sreact.strict_mode"
8:"$Sreact.suspense"
9:I[1646,["185","static/chunks/app/layout-9e246afea995a27e.js"],"default"]
3:T1977,<p>Our research seeks to build towards the following goals:</p>
<ul>
<li>A science of the space of intelligent systems</li>
<li>Naturalised theories of agency, including theories that account for the multi-scale nature of agentic behaviour</li>
<li>Nuanced accounts of how humans reason, act and value as it pertains to solving the AI alignment problem</li>
<li>An understanding of the underlying principles that govern collective behaviour between humans, between humans and AIs, and between AI systems</li>
</ul>
<h2>A science of the space of intelligent systems</h2>
<p>We seek to build systematic understanding of which properties of intelligent behaviour are universal, convergent or local across a wide range of systems, scales and substrates. This understanding forms the basis for asking the right sorts of questions about the risks, potentials and design imperatives of advanced AI systems.</p>
<p>To this end, we draw on a range of sophisticated thinking that has already been done, including in evolutionary biology, cognitive science, statistical physics, economics, ecology, cybernetics and information theory. By integrating and build on these traditions, we aim to better understand the trajectory space for advanced AI systems.</p>
<h2>Hierarchical Agency</h2>
<p>Over the past 100 or so years, a large amount of maths has been developed (most of it under the name of Game Theory) to help us describe the <em>relations between agents at the same level of analysis.</em> At the same time, many systems have several levels of analysis at which their behaviour could be sensibly described and explained. For instance, we can usefully model a company as an agent, or its employees; a social movement, or its followers; a nation-state, or its political class; etc.</p>
<p>At ACS, we aim to develop a conceptual framework, which can help us reason about the <em>relations between agents at different levels of analysis,</em> i.e. between superagents and their subagents. What we want is a formalism which is good for thinking about both upward and downward intentionality - something akin to ‘vertical game theory’ or a ‘theory of hierarchical agency’. We believe this understanding is critical to building and integrating AI systems into human socio-economic and political structures in a way that is safe and aligned.</p>
<p>Our current research in this direction is inspired by ideas originating in the field of <em>active inference,</em> which we extending to multi-agent settings, in order to model complex systems, cooperation, and bounded group rationality.</p>
<h2>Realistic Accounts of how Humans Reason and Value</h2>
<p>‘Alignment’ or ‘safety’ are properties defined at the interface between a system and its environment, rather than properties intrinsic to a system in isolation. As such, it matters to understand the structural and functional properties of both the system that is to be aligned, as well as what we are aligning it to. However, we could pick out several plausible candidates as targets for what we want to align AI systems to–from individual humans to human groups. In addition, humans, importantly, are not accurately described by fixed or latent utility and belief functions, as the classical rational agent model suggests. Overall, current theorising on the appropriate targets of alignment and their structural properties is inadequate for understanding the subtleties that arise when tackling the problem of AI alignment.</p>
<p>ACS works on developing AI alignment proposals which are based on a realistic understanding of how humans reason and value in practice and which recognize the hierarchical relationships between these target systems is critical to solving the problem and not something which can be postponed or delegated to AI systems.</p>
<h2>Ecosystems of Intelligence</h2>
<p>Consider not just one powerful AGI, or several, but an entire ecosystem of different AI and human-AI systems and services varying in power, specialisation, and agency. From here, we can investigate questions about convergent or contingent properties of such ecosystems, what shapes their trajectory through time, as well as their strategic implications. For example, a period where human-AI teams are more capable than either humans or AI systems on their own might provide a critical window during which we can shape the nature of subsequent, more powerful systems and the institutions and protocols which shape their interactions. ACS’s work explores problems of AI risk and safety from the ecosystems perspective.</p>
<h2>Our Publications</h2>
<p><strong>Jan Kulveit</strong>, <strong>Clem von Stengel</strong>, Roman Leventov: <em>Predictive Minds: LLMs As Atypical Active Inference Agents.</em> December 2023, <a href="https://openreview.net/forum?id=bak7hB0Zv9">NeurIPS 2023 SoLaR workshop</a>, <a href="https://arxiv.org/abs/2311.10215">arXiv</a></p>
<p><strong>Nora Ammann</strong>: <em>Value Malleability and its implication for AI alignment.</em> December 2023, NeurIPS 2023 MP<sup>2</sup> workshop</p>
<p>Hardik Rajpal, <strong>Clem von Stengel</strong>, Pedro A. M. Mediano, Fernando E. Rosas, Eduardo Viegas, Pablo A. Marquet, Henrik J. Jensen: <em>Quantifying Hierarchical Selection.</em> November 2023, <a href="https://arxiv.org/abs/2310.20386">arXiv</a></p>
<p><strong>Nora Ammann</strong>, <strong>Clem von Stengel</strong>: <em>A Naturalised Account of Planning in Intelligent Systems.</em> July 2023, <a href="https://direct.mit.edu/isal/proceedings/isal/35/138/116942">Proceedings of ALIFE 2023</a></p>
<!-- Raymond Douglas, Andis Draguns, **Tomáš Gavenčiak**: *Mitigating the Problem of Strong Priors in LMs with Context Extrapolation.* January 2024, [arXiv](https://arxiv.org/abs/2401.17692) -->
<p>Walter Laurito, Benjamin Davis, Peli Grietzer, <strong>Tomáš Gavenčiak</strong>, <strong>Ada Böhm</strong>, <strong>Jan Kulveit</strong>: <em>AI AI Bias: Large Language Models Favor Their Own Generated Content.</em> July 2024, <a href="https://arxiv.org/abs/2407.12856">arXiv</a> and ICML 2024 HADMS workshop</p>
<p>David Hyland, <strong>Tomáš Gavenčiak</strong>, Lancelot Da Costa, Conor Heins, Vojtech Kovarik, Julian Gutierrez, Michael J. Wooldridge, <strong>Jan Kulveit</strong>: <em>Free-Energy Equilibria: Toward a Theory of Interactions Between Boundedly-Rational Agents.</em> July 2024, <a href="https://openreview.net/pdf?id=4Ft7DcrjdO">ICML 2024 MHFAIA workshop</a>, <a href="media/FEE-ICML-2024-poster.pdf">poster</a></p>5:["path","research","c"]
0:["hj5p4X8fWz3Sh-MNmH9Hw",[[["",{"children":[["path","research","c"],{"children":["__PAGE__?{\"path\":[\"research\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["path","research","c"],{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"h-screen","children":[["$","$L2",null,{"root":false}],["$","div",null,{"className":"container prose Content_content__bGfAW","children":[["$","h1",null,{"className":"pt-8","children":"ACS Research Program"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$3"}}]]}],["$","footer",null,{"className":"footer-container","children":[["$","div",null,{"className":"footer-divider"}],["$","div",null,{"className":"footer-column-logo","children":["$","img",null,{"className":"footer-logo","src":"/cuni.png","alt":"CUNI Logo"}]}],["$","div",null,{"className":"footer-columns","children":[["$","div",null,{"className":"footer-subcolumn","children":["ACS research group is part of"," ",["$","a",null,{"href":"http://www.cts.cuni.cz/en","children":"Center for Theoretical Study"}]," at"," ",["$","a",null,{"href":"https://cuni.cz/en","children":"Charles University in Prague."}]]}],["$","div",null,{"className":"footer-subcolumn","children":[["$","div",null,{"className":"footer-contact-line","children":[["$","span",null,{"style":{"float":"left","padding":"0 0.5em 1.3em 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4L236.8 313.6c11.4 8.5 27 8.5 38.4 0L492.8 150.4c12.1-9.1 19.2-23.3 19.2-38.4c0-26.5-21.5-48-48-48H48zM0 176V384c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V176L294.4 339.2c-22.8 17.1-54 17.1-76.8 0L0 176z"}]}]}],"Husova 4",["$","br",null,{}],"110 00 Prague, CZ"]}],["$","div",null,{"className":"footer-contact-line","children":["$","a",null,{"href":"mailto:contact@acsresearch.org","children":[["$","span",null,{"style":{"padding":"0 0.5em 0 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M256 64C150 64 64 150 64 256s86 192 192 192c17.7 0 32 14.3 32 32s-14.3 32-32 32C114.6 512 0 397.4 0 256S114.6 0 256 0S512 114.6 512 256v32c0 53-43 96-96 96c-29.3 0-55.6-13.2-73.2-33.9C320 371.1 289.5 384 256 384c-70.7 0-128-57.3-128-128s57.3-128 128-128c27.9 0 53.7 8.9 74.7 24.1c5.7-5 13.1-8.1 21.3-8.1c17.7 0 32 14.3 32 32v80 32c0 17.7 14.3 32 32 32s32-14.3 32-32V256c0-106-86-192-192-192zm64 192a64 64 0 1 0 -128 0 64 64 0 1 0 128 0z"}]}]}],"contact@acsresearch.org"]}]}],["$","div",null,{"className":"footer-contact-line","children":["$","a",null,{"href":"https://twitter.com/acsresearchorg","children":[["$","span",null,{"style":{"padding":"0 0.5em 0 0"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 512 512","style":{"display":"inline","height":"1em"},"children":["$","path",null,{"d":"M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"}]}]}],"@acsresearchorg"]}]}]]}]]}]]}]]}]],null],null]},["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/c70e259a19bc14f0.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},[["$","html",null,{"lang":"en","children":["$","$7",null,{"children":["$","body",null,{"className":"font-serif","children":[["$","$8",null,{"children":["$","$L9",null,{}]}],["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a6a7d77a57a1ff75.css","precedence":"next","crossOrigin":"$undefined"}]],"$La"]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Alignment of Complex Systems Research Group – ACS Research Program"}],["$","link","3",{"rel":"icon","href":"/icon.png"}]]
1:null
